# configs/ppo_hopper.yaml
# Hyperparameters for Proximal Policy Optimization (PPO) on Hopper-v4
algo:
  name: "PPO"                     # 算法名称
  type: "policy-based"                     # 算法类型 (Proximal Policy Optimization)
# -- Environment Settings --
env:
  id: "MuJoCo"                # MuJoCo 环境 ID

# -- Training Settings --
training:
  seed: 42                       # 随机种子
  device: "cuda"                 # 训练设备
  total_timesteps: 1000000       # 总训练步数 (1M is standard for MuJoCo)    
  save_interval: 200000

# -- Agent Hyperparameters --
agent:
  gamma: 0.99                    # 折扣因子
  lr: 0.0003                     # 学习率 (通常为常数或线性衰减)
  gae_lambda: 0.95               # GAE (Generalized Advantage Estimation) 的 lambda 参数
  hidden_sizes: [64, 64]       # 策略网络和价值网络的隐藏层大小 (可以根据需要调整)
  # -- PPO Specific Parameters --
  n_steps: 2048                  # 每次更新前，每个环境收集的步数 (Rollout buffer size)
  n_epochs: 10                   # 每次更新时，在收集到的数据上训练的轮数
  batch_size: 64                 # mini-batch 大小
  clip_epsilon: 0.2              # PPO 裁剪目标函数的裁剪范围
  shared: False
  # -- Loss Coefficients --
  vf_coef: 0.5                   # 价值函数损失的系数 (Value function loss coefficient)
  ent_coef: 0.0                  # 熵损失的系数 (Entropy bonus coefficient)
  
  # -- Other --
  max_grad_norm: 0.5             # 梯度裁剪的最大范数，防止梯度爆炸

# -- Logging --
logging:
  log_interval: 2048             # 每收集一次 rollout (n_steps) 记录一次日志
  save_interval: 50000           # 每隔多少步保存一次模型