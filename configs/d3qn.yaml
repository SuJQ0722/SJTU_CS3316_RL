# configs/d3qn_breakout.yaml
# Hyperparameters for Dueling Double DQN (D3QN) on BreakoutNoFrameskip-v4
algo:
  name: "D3QN"                    # 算法名称
  type: "value-based"                     # 算法类型 (DQN, Dueling DQN, Double DQN, etc.)
# -- Environment Settings --
env:
  id: "Atari" # Atari 环境 ID
  frame_stack: 4               # 堆叠的帧数，让智能体感知动态
  max_episode_steps: 2000
# -- Training Settings --
training:
  seed: 42                       # 随机种子，用于复现实验结果
  device: "cuda"                 # 训练设备, "cuda" or "cpu"
  total_timesteps: 1000000      # 总训练步数 (1M for Atari is a good start)
  
# -- Agent Hyperparameters --
agent:
  gamma: 0.99                    # 折扣因子 (Discount factor)
  buffer_size: 100000            # 经验回放池大小 (Replay buffer size, 100k to save memory, 1M is standard)
  batch_size: 32                 # 每次更新时采样的批量大小 (Batch size for learning)
  lr: 0.0001                     # 学习率 (Learning rate for Adam optimizer)
  
  # -- Q-Network Update Rules --
  target_update_interval: 1000   # 目标网络更新频率 (以训练步数为单位)
  learning_starts: 10000         # 在收集这么多步经验后才开始训练

  # -- Epsilon-Greedy Exploration --
  eps_start: 1.0                 # 探索率初始值
  eps_end: 0.01                  # 探索率最终值
  eps_fraction: 0.1              # 探索率从 start 衰减到 end 所占总步数的比例

# -- Logging --
logging:
  log_interval: 10000            # 每隔多少步记录一次日志 (e.g., to TensorBoard)
  save_interval: 100000          # 每隔多少步保存一次模型